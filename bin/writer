#!/usr/bin/env python

import logging
import simplejson as json
import signal
import time

import click
from clickhouse import errors
from raven import Client as RavenClient

from snuba import settings
from snuba.clickhouse import Clickhouse
from snuba.consumer import AbstractBatchWorker, BatchingKafkaConsumer
from snuba.writer import row_from_processed_event, write_rows
from snuba import util


logger = logging.getLogger('snuba.writer')
sentry = RavenClient(dsn=settings.SENTRY_DSN)


@click.command()
@click.option('--processed-events-topic', default='snuba',
              help='Topic to consume processed events from.')
@click.option('--consumer-group', default='snuba-writers',
              help='Consumer group used for consuming the processed events topic.')
@click.option('--bootstrap-server', default=settings.DEFAULT_BROKERS, multiple=True,
              help='Kafka bootstrap server to use (multiple allowed).')
@click.option('--clickhouse-server', default=settings.CLICKHOUSE_SERVER,
              help='Clickhouse server to write to.')
@click.option('--local-table-name', default=settings.DEFAULT_LOCAL_TABLE,
              help='Clickhouse table name for the (optionally replicated) local table.')
@click.option('--distributed-table-name', default=settings.DEFAULT_DIST_TABLE,
              help='Clickhouse table name for the "meta" Distributed table.')
@click.option('--log-level', default=settings.LOG_LEVEL, help='Logging level to use.')
@click.option('--dogstatsd-host', default='localhost', help='Host to send DogStatsD metrics to.')
@click.option('--dogstatsd-port', default=8125, type=int, help='Port to send DogStatsD metrics to.')
def run(processed_events_topic, consumer_group, bootstrap_server, clickhouse_server,
        local_table_name, distributed_table_name, log_level, dogstatsd_host, dogstatsd_port):

    logging.basicConfig(level=getattr(logging, log_level.upper()), format='%(asctime)s %(message)s')
    metrics = util.create_metrics(dogstatsd_host, dogstatsd_port, 'snuba.writer')
    clickhouse = Clickhouse(clickhouse_server.split(':')[0], port=int(clickhouse_server.split(':')[1]))

    # ensure tables exist
    LOCAL_TABLE_DEFINITION = util.get_table_definition(
        local_table_name, util.get_replicated_engine(name=local_table_name))
    DIST_TABLE_DEFINITION = util.get_table_definition(distributed_table_name, util.get_distributed_engine(
        cluster='cluster1',
        database='default',
        local_table=local_table_name,
    ))

    with clickhouse as ch:
        ch.execute(LOCAL_TABLE_DEFINITION)
        ch.execute(DIST_TABLE_DEFINITION)

    class WriterWorker(AbstractBatchWorker):
        def process_message(self, message):
            return row_from_processed_event(json.loads(message.value))

        def flush_batch(self, batch):
            for attempt in xrange(2):
                try:
                    with clickhouse as ch:
                        write_rows(ch, distributed_table_name, settings.WRITER_COLUMNS, batch)

                    break
                except (errors.NetworkError, errors.SocketTimeoutError) as e:
                    if attempt == 0:
                        logger.exception("Error writing to Clickhouse, retrying.")
                        time.sleep(1)
                    else:
                        logger.exception("Error writing to Clickhouse, giving up.")

        def shutdown(self):
            pass

    consumer = BatchingKafkaConsumer(
        processed_events_topic,
        worker=WriterWorker(),
        max_batch_size=25000,
        max_batch_time=30 * 1000,
        metrics=metrics,
        # KafkaConsumer
        bootstrap_servers=bootstrap_server,
        group_id=consumer_group,
        auto_offset_reset='earliest',
        consumer_timeout_ms=1000,
    )

    def handler(signum, frame):
        consumer.signal_shutdown()

    signal.signal(signal.SIGINT, handler)

    consumer.run()


if __name__ == '__main__':
    run()
