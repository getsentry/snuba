#!/usr/bin/env python

import logging
import simplejson as json
import signal

import click
from raven import Client as RavenClient

from snuba import settings
from snuba.clickhouse import Clickhouse
from snuba.consumer import AbstractBatchWorker, BatchingKafkaConsumer
from snuba.writer import row_from_processed_event, write_rows
from snuba import util


logger = logging.getLogger('snuba.writer')
sentry = RavenClient(dsn=settings.SENTRY_DSN)


@click.command()
@click.option('--processed-events-topic', default='snuba',
              help='Topic to consume processed events from.')
@click.option('--consumer-group', default='snuba-writers',
              help='Consumer group used for consuming the processed events topic.')
@click.option('--bootstrap-server', default=settings.DEFAULT_BROKERS, multiple=True,
              help='Kafka bootstrap server to use (multiple allowed).')
@click.option('--clickhouse-server', default=settings.CLICKHOUSE_SERVER,
              help='Clickhouse server to write to.')
@click.option('--local-table-name', default=settings.DEFAULT_LOCAL_TABLE,
              help='Clickhouse table name for the (optionally replicated) local table.')
@click.option('--distributed-table-name', default=settings.DEFAULT_DIST_TABLE,
              help='Clickhouse table name for the "meta" Distributed table.')
@click.option('--log-level', default=settings.LOG_LEVEL, help='Logging level to use.')
@click.option('--dogstatsd-host', default='localhost', help='Host to send DogStatsD metrics to.')
@click.option('--dogstatsd-port', default=8125, type=int, help='Port to send DogStatsD metrics to.')
def run(processed_events_topic, consumer_group, bootstrap_server, clickhouse_server,
        local_table_name, distributed_table_name, log_level, dogstatsd_host, dogstatsd_port):

    logging.basicConfig(level=getattr(logging, log_level.upper()), format='%(asctime)s %(message)s')
    metrics = util.create_metrics(dogstatsd_host, dogstatsd_port, 'snuba.writer')
    clickhouse = Clickhouse(clickhouse_server.split(':')[0], port=int(clickhouse_server.split(':')[1]))

    # ensure tables exist
    LOCAL_TABLE_DEFINITION = util.get_table_definition(
        local_table_name, util.get_replicated_engine(name=local_table_name))
    DIST_TABLE_DEFINITION = util.get_table_definition(distributed_table_name, util.get_distributed_engine(
        cluster='cluster1',
        database='default',
        local_table=local_table_name,
    ))

    with clickhouse as ch:
        ch.execute(LOCAL_TABLE_DEFINITION)
        ch.execute(DIST_TABLE_DEFINITION)

    class WriterWorker(AbstractBatchWorker):
        def process_message(self, message):
            return row_from_processed_event(json.loads(message.value))

        def flush_batch(self, batch):
            with clickhouse as ch:
                write_rows(ch, distributed_table_name, settings.WRITER_COLUMNS, batch)

        def shutdown(self):
            pass

    consumer = BatchingKafkaConsumer(
        processed_events_topic,
        worker=WriterWorker(),
        max_batch_size=25000,
        max_batch_time=30 * 1000,
        metrics=metrics,
        # KafkaConsumer
        bootstrap_servers=bootstrap_server,
        group_id=consumer_group,
        auto_offset_reset='earliest',
        consumer_timeout_ms=1000,
    )

    def handler(signum, frame):
        consumer.signal_shutdown()

    signal.signal(signal.SIGINT, handler)

    consumer.run()


if __name__ == '__main__':
    run()
