#!/usr/bin/env python

import logging
import signal

import click
from raven import Client as RavenClient

from snuba import settings
from snuba.clickhouse import (
    Clickhouse, get_table_definition, get_replicated_engine, get_distributed_engine
)
from snuba.consumer import BatchingKafkaConsumer
from snuba.writer import WriterWorker
from snuba import util


sentry = RavenClient(dsn=settings.SENTRY_DSN)


@click.command()
@click.option('--processed-topic', default=['snuba'], multiple=True,
              help='Topic to consume processed messages from.')
@click.option('--consumer-group', default='snuba-writers',
              help='Consumer group used for consuming the processed events topic.')
@click.option('--bootstrap-server', default=settings.DEFAULT_BROKERS, multiple=True,
              help='Kafka bootstrap server to use (multiple allowed).')
@click.option('--clickhouse-server', default=settings.CLICKHOUSE_SERVER,
              help='Clickhouse server to write to.')
@click.option('--local-table-name', default=settings.DEFAULT_LOCAL_TABLE,
              help='Clickhouse table name for the (optionally replicated) local table.')
@click.option('--distributed-table-name', default=settings.DEFAULT_DIST_TABLE,
              help='Clickhouse table name for the "meta" Distributed table.')
@click.option('--max-batch-size', default=settings.DEFAULT_MAX_BATCH_SIZE,
              help='Max number of messages to batch in memory before writing to Clickhouse.')
@click.option('--max-batch-time-ms', default=settings.DEFAULT_MAX_BATCH_TIME_MS,
              help='Max length of time to buffer messages in memory before writing to Clickhouse.')
@click.option('--log-level', default=settings.LOG_LEVEL, help='Logging level to use.')
@click.option('--dogstatsd-host', default=settings.DOGSTATSD_HOST, help='Host to send DogStatsD metrics to.')
@click.option('--dogstatsd-port', default=settings.DOGSTATSD_PORT, type=int, help='Port to send DogStatsD metrics to.')
def run(processed_topic, consumer_group, bootstrap_server, clickhouse_server,
        local_table_name, distributed_table_name, max_batch_size, max_batch_time_ms,
        log_level, dogstatsd_host, dogstatsd_port):

    logging.basicConfig(level=getattr(logging, log_level.upper()), format='%(asctime)s %(message)s')
    metrics = util.create_metrics(dogstatsd_host, dogstatsd_port, 'snuba.writer')
    clickhouse = Clickhouse(clickhouse_server.split(':')[0], port=int(clickhouse_server.split(':')[1]))

    # ensure tables exist
    LOCAL_TABLE_DEFINITION = get_table_definition(
        local_table_name, get_replicated_engine(name=local_table_name))
    DIST_TABLE_DEFINITION = get_table_definition(
        distributed_table_name,
        get_distributed_engine(
            cluster='cluster1',
            database='default',
            local_table=local_table_name,
        )
    )

    with clickhouse as ch:
        ch.execute(LOCAL_TABLE_DEFINITION)
        ch.execute(DIST_TABLE_DEFINITION)

    consumer = BatchingKafkaConsumer(
        processed_topic,
        worker=WriterWorker(clickhouse, distributed_table_name),
        max_batch_size=max_batch_size,
        max_batch_time=max_batch_time_ms,
        metrics=metrics,
        bootstrap_servers=bootstrap_server,
        group_id=consumer_group,
    )

    def handler(signum, frame):
        consumer.signal_shutdown()

    signal.signal(signal.SIGINT, handler)

    consumer.run()


if __name__ == '__main__':
    run()
