#!/usr/bin/env python

import logging
import simplejson as json
import signal
import time

import click
from clickhouse_driver import errors
from raven import Client as RavenClient

from snuba import settings
from snuba.clickhouse import Clickhouse
from snuba.consumer import AbstractBatchWorker, BatchingKafkaConsumer
from snuba.writer import row_from_processed_event, write_rows
from snuba import util


logger = logging.getLogger('snuba.writer')
sentry = RavenClient(dsn=settings.SENTRY_DSN)


@click.command()
@click.option('--processed-events-topic', default='snuba',
              help='Topic to consume processed events from.')
@click.option('--consumer-group', default='snuba-writers',
              help='Consumer group used for consuming the processed events topic.')
@click.option('--bootstrap-server', default=settings.DEFAULT_BROKERS, multiple=True,
              help='Kafka bootstrap server to use (multiple allowed).')
@click.option('--clickhouse-server', default=settings.CLICKHOUSE_SERVER,
              help='Clickhouse server to write to.')
@click.option('--local-table-name', default=settings.DEFAULT_LOCAL_TABLE,
              help='Clickhouse table name for the (optionally replicated) local table.')
@click.option('--distributed-table-name', default=settings.DEFAULT_DIST_TABLE,
              help='Clickhouse table name for the "meta" Distributed table.')
@click.option('--max-batch-size', default=settings.DEFAULT_MAX_BATCH_SIZE,
              help='Max number of messages to batch in memory before writing to Clickhouse.')
@click.option('--max-batch-time-ms', default=settings.DEFAULT_MAX_BATCH_TIME_MS,
              help='Max length of time to buffer messages in memory before writing to Clickhouse.')
@click.option('--log-level', default=settings.LOG_LEVEL, help='Logging level to use.')
@click.option('--dogstatsd-host', default=settings.DOGSTATSD_HOST, help='Host to send DogStatsD metrics to.')
@click.option('--dogstatsd-port', default=settings.DOGSTATSD_PORT, type=int, help='Port to send DogStatsD metrics to.')
def run(processed_events_topic, consumer_group, bootstrap_server, clickhouse_server,
        local_table_name, distributed_table_name, max_batch_size, max_batch_time_ms,
        log_level, dogstatsd_host, dogstatsd_port):

    logging.basicConfig(level=getattr(logging, log_level.upper()), format='%(asctime)s %(message)s')
    metrics = util.create_metrics(dogstatsd_host, dogstatsd_port, 'snuba.writer')
    clickhouse = Clickhouse(clickhouse_server.split(':')[0], port=int(clickhouse_server.split(':')[1]))

    # ensure tables exist
    LOCAL_TABLE_DEFINITION = util.get_table_definition(
        local_table_name, util.get_replicated_engine(name=local_table_name))
    DIST_TABLE_DEFINITION = util.get_table_definition(distributed_table_name, util.get_distributed_engine(
        cluster='cluster1',
        database='default',
        local_table=local_table_name,
    ))

    with clickhouse as ch:
        ch.execute(LOCAL_TABLE_DEFINITION)
        ch.execute(DIST_TABLE_DEFINITION)

    class WriterWorker(AbstractBatchWorker):
        def process_message(self, message):
            return row_from_processed_event(json.loads(message.value()))

        def flush_batch(self, batch):
            retries = 3
            while True:
                try:
                    with clickhouse as ch:
                        write_rows(ch, distributed_table_name, settings.WRITER_COLUMNS, batch)

                    break  # success
                except (errors.NetworkError, errors.SocketTimeoutError) as e:
                    logger.warning("Write to Clickhouse failed: %s (%d retries)" % (str(e), retries))
                    if retries <= 0:
                        raise
                    retries -= 1
                    time.sleep(1)
                    continue
                except errors.ServerException as e:
                    logger.warning("Write to Clickhouse failed: %s (retrying)" % str(e))
                    if e.code == errors.ErrorCodes.TOO_MUCH_SIMULTANEOUS_QUERIES:
                        time.sleep(1)
                        continue
                    else:
                        raise

        def shutdown(self):
            pass

    consumer = BatchingKafkaConsumer(
        processed_events_topic,
        worker=WriterWorker(),
        max_batch_size=max_batch_size,
        max_batch_time=max_batch_time_ms,
        metrics=metrics,
        bootstrap_servers=bootstrap_server,
        group_id=consumer_group,
    )

    def handler(signum, frame):
        consumer.signal_shutdown()

    signal.signal(signal.SIGINT, handler)

    consumer.run()


if __name__ == '__main__':
    run()
