#!/usr/bin/env python

import logging
import simplejson as json
import signal

import click
from kafka import KafkaProducer
from raven import Client as RavenClient

from snuba import settings, util
from snuba.consumer import AbstractBatchWorker, BatchingKafkaConsumer
from snuba.processor import get_key, process_raw_event


logger = logging.getLogger('snuba.processor')
sentry = RavenClient(dsn=settings.SENTRY_DSN)


@click.command()
@click.option('--raw-events-topic', default='events',
              help='Topic to consume raw events from.')
@click.option('--consumer-group', default='snuba-processors',
              help='Consumer group use for consuming the raw events topic.')
@click.option('--processed-events-topic', default='snuba',
              help='Topic to send processed events to.')
@click.option('--bootstrap-server', default=settings.DEFAULT_BROKERS, multiple=True,
              help='Kafka bootstrap server to use.')
@click.option('--max-batch-size', default=settings.DEFAULT_MAX_BATCH_SIZE,
              help='Max number of messages to batch in memory before writing to Kafka.')
@click.option('--max-batch-time-ms', default=settings.DEFAULT_MAX_BATCH_TIME_MS,
              help='Max length of time to buffer messages in memory before writing to Kafka.')
@click.option('--log-level', default=settings.LOG_LEVEL, help='Logging level to use.')
@click.option('--dogstatsd-host', default=settings.DOGSTATSD_HOST, help='Host to send DogStatsD metrics to.')
@click.option('--dogstatsd-port', default=settings.DOGSTATSD_PORT, type=int, help='Port to send DogStatsD metrics to.')
def run(raw_events_topic, consumer_group, processed_events_topic, bootstrap_server,
        max_batch_size, max_batch_time_ms, log_level, dogstatsd_host, dogstatsd_port):

    logging.basicConfig(level=getattr(logging, log_level.upper()), format='%(asctime)s %(message)s')
    metrics = util.create_metrics(dogstatsd_host, dogstatsd_port, 'snuba.processor')

    producer = KafkaProducer(bootstrap_servers=bootstrap_server)

    class ProcessorWorker(AbstractBatchWorker):
        def process_message(self, message):
            event = json.loads(message.value())

            key = get_key(event).encode('utf-8')
            processed = json.dumps(process_raw_event(event)).encode('utf-8')

            return (key, processed)

        def flush_batch(self, batch):
            for key, value in batch:
                producer.send(
                    processed_events_topic,
                    key=key,
                    value=value,
                )

            producer.flush()

        def shutdown(self):
            producer.flush()
            producer.close()

    consumer = BatchingKafkaConsumer(
        raw_events_topic,
        worker=ProcessorWorker(),
        max_batch_size=max_batch_size,
        max_batch_time=max_batch_time_ms,
        metrics=metrics,
        bootstrap_server=bootstrap_server[0],
        group_id=consumer_group,
    )

    def handler(signum, frame):
        consumer.signal_shutdown()

    signal.signal(signal.SIGINT, handler)

    consumer.run()


if __name__ == '__main__':
    run()
