#!/usr/bin/env python

import logging
import simplejson as json
import signal

import click
from kafka import KafkaProducer
from raven import Client as RavenClient

from snuba import settings, util
from snuba.consumer import AbstractBatchWorker, BatchingKafkaConsumer
from snuba.processor import get_key, process_raw_event


logger = logging.getLogger('snuba.processor')
sentry = RavenClient(dsn=settings.SENTRY_DSN)


@click.command()
@click.option('--raw-events-topic', default='events',
              help='Topic to consume raw events from.')
@click.option('--consumer-group', default='snuba-processors',
              help='Consumer group use for consuming the raw events topic.')
@click.option('--processed-events-topic', default='snuba',
              help='Topic to send processed events to.')
@click.option('--bootstrap-server', default=settings.DEFAULT_BROKERS, multiple=True,
              help='Kafka bootstrap server to use.')
@click.option('--log-level', default=settings.LOG_LEVEL, help='Logging level to use.')
@click.option('--dogstatsd-host', default='localhost', help='Host to send DogStatsD metrics to.')
@click.option('--dogstatsd-port', default=8125, type=int, help='Port to send DogStatsD metrics to.')
def run(raw_events_topic, consumer_group, processed_events_topic, bootstrap_server, log_level,
        dogstatsd_host, dogstatsd_port):

    logging.basicConfig(level=getattr(logging, log_level.upper()))
    metrics = util.create_metrics(dogstatsd_host, dogstatsd_port, 'snuba.processor')

    producer = KafkaProducer(bootstrap_servers=bootstrap_server)

    class ProcessorWorker(AbstractBatchWorker):
        def process_message(self, message):
            event = json.loads(message.value)

            key = get_key(event).encode('utf-8')
            processed = json.dumps(process_raw_event(event)).encode('utf-8')

            return (key, processed)

        def flush_batch(self, batch):
            for key, value in batch:
                producer.send(
                    processed_events_topic,
                    key=key,
                    value=value,
                )

            producer.flush()

        def shutdown(self):
            producer.flush()
            producer.close()

    consumer = BatchingKafkaConsumer(
        raw_events_topic,
        worker=ProcessorWorker(),
        max_batch_size=50000,
        max_batch_time=5000,
        metrics=metrics,
        # KafkaConsumer
        bootstrap_servers=bootstrap_server,
        group_id=consumer_group,
        auto_offset_reset='earliest',
        consumer_timeout_ms=1000,
    )

    def handler(signum, frame):
        consumer.signal_shutdown()

    signal.signal(signal.SIGINT, handler)

    consumer.run()


if __name__ == '__main__':
    run()
