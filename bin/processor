#!/usr/bin/env python

"""
The Processor consumes the Sentry raw events topic and produces fully
"processed" inserts and deletes to different Kafka topics. "Processed" means
they are ready for insertion into Clickhouse by downstream writers.

Inserts and deletes can be produced to the same topic or different ones.
Ordering of inserts and deletes is not a concern because we use a
ReplacingMergeTree that is versioned on the `deleted` column, so deletes will
always win during a merge, even if they were written before their matching
event. The reason we allow inserts and deletes to be split up is that different
sets of downstream writers can work on different sets of messages, which
prevents a large project deletion from backing up insert writes. In a way, the
two topics can be thought of as priority levels.
"""

import logging
import signal

import click
from confluent_kafka import Producer
from raven import Client as RavenClient

from snuba import settings, util
from snuba.consumer import BatchingKafkaConsumer
from snuba.processor import ProcessorWorker


sentry = RavenClient(dsn=settings.SENTRY_DSN)


@click.command()
@click.option('--raw-events-topic', default='events',
              help='Topic to consume raw events from.')
@click.option('--consumer-group', default='snuba-processors',
              help='Consumer group use for consuming the raw events topic.')
@click.option('--processed-events-topic', default='snuba',
              help='Topic to send processed events to.')
@click.option('--processed-deletes-topic', default='snuba',
              help='Topic to send processed deletes to.')
@click.option('--bootstrap-server', default=settings.DEFAULT_BROKERS, multiple=True,
              help='Kafka bootstrap server to use.')
@click.option('--max-batch-size', default=settings.DEFAULT_MAX_BATCH_SIZE,
              help='Max number of messages to batch in memory before writing to Kafka.')
@click.option('--max-batch-time-ms', default=settings.DEFAULT_MAX_BATCH_TIME_MS,
              help='Max length of time to buffer messages in memory before writing to Kafka.')
@click.option('--auto-offset-reset', default='error', type=click.Choice(['error', 'earliest', 'latest']),
              help='Kafka consumer auto offset reset.')
@click.option('--log-level', default=settings.LOG_LEVEL, help='Logging level to use.')
@click.option('--dogstatsd-host', default=settings.DOGSTATSD_HOST, help='Host to send DogStatsD metrics to.')
@click.option('--dogstatsd-port', default=settings.DOGSTATSD_PORT, type=int, help='Port to send DogStatsD metrics to.')
def run(raw_events_topic, consumer_group, processed_events_topic, processed_deletes_topic,
        bootstrap_server, max_batch_size, max_batch_time_ms, auto_offset_reset, log_level,
        dogstatsd_host, dogstatsd_port):

    logging.basicConfig(level=getattr(logging, log_level.upper()), format='%(asctime)s %(message)s')
    metrics = util.create_metrics(dogstatsd_host, dogstatsd_port, 'snuba.processor')

    producer = Producer({'bootstrap.servers': ','.join(bootstrap_server)})

    consumer = BatchingKafkaConsumer(
        raw_events_topic,
        worker=ProcessorWorker(producer, processed_events_topic, processed_deletes_topic),
        max_batch_size=max_batch_size,
        max_batch_time=max_batch_time_ms,
        metrics=metrics,
        bootstrap_servers=bootstrap_server,
        group_id=consumer_group,
        auto_offset_reset=auto_offset_reset,
    )

    def handler(signum, frame):
        consumer.signal_shutdown()

    signal.signal(signal.SIGINT, handler)

    consumer.run()


if __name__ == '__main__':
    run()
