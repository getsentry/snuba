#!/usr/bin/env python
import argparse
import os
from datetime import datetime, timedelta

from arroyo import Partition, Topic
from arroyo.backends.kafka import KafkaPayload
from arroyo.backends.local.backend import LocalBroker as Broker
from arroyo.backends.local.storages.abstract import TopicExists
from arroyo.backends.local.storages.file import FileMessageStorage
from arroyo.synchronized import Commit, commit_codec

from snuba.datasets.entities import EntityKey
from snuba.datasets.entities.factory import get_entity

parser = argparse.ArgumentParser(description="Mock commit log data")
parser.add_argument(
    "--entity",
    type=str,
    help="Entity's commit log to write data to",
    dest="entity",
    choices=["events"],
    default="events",
)
parser.add_argument(
    "--number",
    type=int,
    help="Number of commit log ticks to write",
    dest="number",
    required=True,
)
parser.add_argument(
    "--group-id",
    type=str,
    help="Consumer group ID",
    dest="group_id",
    default="snuba-consumers",
)

parsed = parser.parse_args()
entity_name = parsed.entity
number = parsed.number
consumer_group = parsed.group_id

entity = get_entity(EntityKey(entity_name))
storage = entity.get_writable_storage()
assert storage is not None
stream_loader = storage.get_table_writer().get_stream_loader()
commit_log_topic_spec = stream_loader.get_commit_log_topic_spec()
assert commit_log_topic_spec is not None
topic = Topic(commit_log_topic_spec.topic_name)

# Create the commit log topic
directory_path = os.getcwd() + "/.broker_data"

try:
    os.mkdir(directory_path)
except FileExistsError:
    pass

broker: Broker[KafkaPayload] = Broker(FileMessageStorage(directory_path))

try:
    broker.create_topic(topic, partitions=1)
except TopicExists:
    pass

# Write ticks for every partition starting at 0 and 1 second ago
partitions_number = stream_loader.get_default_topic_spec().partitions_number
partitions = [Partition(topic, i) for i in range(partitions_number)]

now = datetime.now() - timedelta(minutes=2)

for n in range(number):
    offset = n
    orig_message_ts = now - timedelta(seconds=1 * (number - n))

    for partition in partitions:
        commit = Commit(consumer_group, partition, offset, orig_message_ts)

        kafka_payload = commit_codec.encode(commit)

        producer = broker.get_producer()
        producer.produce(partition, kafka_payload).result()
