# Histogram Bucketing Example for EndpointGetTrace

## Visual Guide: Distribution Metrics vs Traditional Timing Metrics

### Before: Traditional Timing Metrics

With traditional `timing` metrics, you only get aggregated statistics:

```
EndpointGetTrace Timing (Traditional)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Metric: endpoint_timing

Available Data:
  â€¢ Average: 450ms
  â€¢ Min: 50ms
  â€¢ Max: 5000ms
  â€¢ P50: 300ms
  â€¢ P95: 1200ms
  â€¢ P99: 2500ms

âŒ Problem: You can't answer questions like:
  â€¢ "How many requests took less than 100ms?"
  â€¢ "What percentage of requests are under 500ms?"
  â€¢ "How is latency distributed across different ranges?"
```

### After: Distribution Metrics with Histogram Bucketing

With `distribution` metrics, you can create custom buckets and count datapoints:

```
EndpointGetTrace Timing Distribution
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Metric: endpoint_timing (as distribution)

Histogram Buckets (1 hour window):
  < 100ms     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 3,200 requests (32%)
  100-250ms   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 4,000 requests (40%)
  250-500ms   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1,600 requests (16%)
  500-1000ms  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 800 requests (8%)
  1s-3s       â–ˆâ–ˆâ–ˆ 300 requests (3%)
  > 3s        â–ˆ 100 requests (1%)

âœ… Now you can answer:
  â€¢ "72% of requests complete under 250ms" âœ“
  â€¢ "Only 1% take longer than 3 seconds" âœ“
  â€¢ "We need to optimize the 8% in the 500-1000ms range" âœ“
```

## DataDog Query Examples

### 1. Count Requests by Latency Bucket

```javascript
// Very Fast: < 100ms
sum:rpc.endpoint_timing{
  endpoint_name:EndpointGetTrace,
  rpc.endpoint_timing < 100
}.as_count()

// Fast: 100-500ms
sum:rpc.endpoint_timing{
  endpoint_name:EndpointGetTrace,
  rpc.endpoint_timing >= 100 AND rpc.endpoint_timing < 500
}.as_count()

// Acceptable: 500ms-1s
sum:rpc.endpoint_timing{
  endpoint_name:EndpointGetTrace,
  rpc.endpoint_timing >= 500 AND rpc.endpoint_timing < 1000
}.as_count()

// Slow: 1s-3s
sum:rpc.endpoint_timing{
  endpoint_name:EndpointGetTrace,
  rpc.endpoint_timing >= 1000 AND rpc.endpoint_timing < 3000
}.as_count()

// Very Slow: > 3s
sum:rpc.endpoint_timing{
  endpoint_name:EndpointGetTrace,
  rpc.endpoint_timing >= 3000
}.as_count()
```

### 2. Calculate Percentage in Each Bucket

```javascript
// Percentage of fast requests (< 500ms)
(
  sum:rpc.endpoint_timing{
    endpoint_name:EndpointGetTrace,
    rpc.endpoint_timing < 500
  }.as_count()
  /
  sum:rpc.endpoint_timing{
    endpoint_name:EndpointGetTrace
  }.as_count()
) * 100
```

### 3. Create Stacked Area Chart

In DataDog Dashboard, create a Stacked Area Chart with these queries:

```javascript
{
  "viz": "area",
  "requests": [
    {
      "q": "sum:rpc.endpoint_timing{endpoint_name:EndpointGetTrace, rpc.endpoint_timing < 100}.as_count()",
      "display_type": "area",
      "style": {"palette": "cool"}
    },
    {
      "q": "sum:rpc.endpoint_timing{endpoint_name:EndpointGetTrace, rpc.endpoint_timing >= 100 AND rpc.endpoint_timing < 500}.as_count()",
      "display_type": "area",
      "style": {"palette": "warm"}
    },
    {
      "q": "sum:rpc.endpoint_timing{endpoint_name:EndpointGetTrace, rpc.endpoint_timing >= 500}.as_count()",
      "display_type": "area",
      "style": {"palette": "orange"}
    }
  ]
}
```

## Real-World Use Case Example

### Scenario: Optimizing GetTrace Performance

**Goal:** Reduce P95 latency from 1200ms to under 1000ms

#### Step 1: Analyze Current Distribution

```
Current State (Distribution View):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EndpointGetTrace Latency Distribution   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ < 100ms:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 20%                â”‚
â”‚ 100-250ms:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 30%            â”‚
â”‚ 250-500ms:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 20%                â”‚
â”‚ 500-1000ms: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 20%                â”‚
â”‚ 1s-2s:      â–ˆâ–ˆâ–ˆ 8%                      â”‚
â”‚ > 2s:       â–ˆ 2%                        â”‚
â”‚                                          â”‚
â”‚ P95: 1200ms                              â”‚
â”‚ Issue: 30% of requests > 500ms           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Analysis:** The distribution shows:
- 70% of requests are reasonably fast (< 500ms)
- 30% of requests are slow (> 500ms)
- There's a clear bimodal distribution

**Action:** Investigate what causes the 30% slow requests.

#### Step 2: Segment by Referrer

```
Distribution by Referrer:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ trace_view referrer:             â”‚
â”‚ < 100ms:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 60%     â”‚
â”‚ 100-500ms:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 38%         â”‚
â”‚ > 500ms:    â–ˆ 2%                 â”‚
â”‚ P95: 250ms âœ“                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ trace_waterfall referrer:        â”‚
â”‚ < 100ms:    â–ˆ 5%                 â”‚
â”‚ 100-500ms:  â–ˆâ–ˆâ–ˆâ–ˆ 15%             â”‚
â”‚ > 500ms:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 80% â”‚
â”‚ P95: 2500ms âŒ                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Discovery:** `trace_waterfall` is the problem referrer!

#### Step 3: Deep Dive into Slow Referrer

```
trace_waterfall by Storage Routing Mode:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NORMAL mode:                     â”‚
â”‚ < 500ms:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 60%     â”‚
â”‚ > 500ms:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 40%         â”‚
â”‚ P95: 800ms                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HIGHEST_ACCURACY mode:           â”‚
â”‚ < 500ms:    â–ˆ 5%                 â”‚
â”‚ > 500ms:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 95%  â”‚
â”‚ P95: 3200ms âŒ                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Root Cause:** `trace_waterfall` uses `HIGHEST_ACCURACY` mode too often!

#### Step 4: Measure Impact of Fix

After optimizing `trace_waterfall`:

```
After Fix (Distribution View):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EndpointGetTrace Latency Distribution   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ < 100ms:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 30%            â”‚
â”‚ 100-250ms:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 30%            â”‚
â”‚ 250-500ms:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 30%            â”‚
â”‚ 500-1000ms: â–ˆâ–ˆâ–ˆâ–ˆ 8%                     â”‚
â”‚ 1s-2s:      â–ˆ 1.5%                      â”‚
â”‚ > 2s:       â–ˆ 0.5%                      â”‚
â”‚                                          â”‚
â”‚ P95: 650ms âœ“ (target achieved!)         â”‚
â”‚ Fast requests (< 500ms): 90% âœ“          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Success Metrics:**
- âœ… P95 reduced from 1200ms â†’ 650ms (46% improvement)
- âœ… Requests < 500ms increased from 70% â†’ 90%
- âœ… Slow requests (> 2s) reduced from 2% â†’ 0.5%

## Bucket Configuration Examples

### Conservative Buckets (Strict SLOs)

```python
BUCKETS = {
    "excellent": "< 100ms",     # ğŸŸ¢ Instant
    "good": "100-250ms",        # ğŸŸ¢ Fast
    "acceptable": "250-500ms",  # ğŸŸ¡ OK
    "slow": "500-1000ms",       # ğŸŸ  Needs attention
    "very_slow": "1s-3s",       # ğŸ”´ Problem
    "timeout_risk": "> 3s"      # ğŸ”´ Critical
}

SLO Targets:
  â€¢ 80% should be "excellent" or "good" (< 250ms)
  â€¢ 95% should be under 1 second
  â€¢ 99% should be under 3 seconds
```

### Aggressive Buckets (Tight Performance Requirements)

```python
BUCKETS = {
    "instant": "< 50ms",        # ğŸŸ¢ Cache hit
    "very_fast": "50-100ms",    # ğŸŸ¢ Hot path
    "fast": "100-200ms",        # ğŸŸ¢ Normal
    "acceptable": "200-500ms",  # ğŸŸ¡ Warm path
    "slow": "500-1000ms",       # ğŸŸ  Cold path
    "unacceptable": "> 1s"      # ğŸ”´ Problem
}

SLO Targets:
  â€¢ 50% should be instant (< 50ms)
  â€¢ 90% should be fast (< 200ms)
  â€¢ 99% should be acceptable (< 500ms)
```

### Realistic Buckets (Balanced Approach)

```python
BUCKETS = {
    "fast": "< 500ms",          # ğŸŸ¢ Good user experience
    "acceptable": "500-1000ms", # ğŸŸ¡ Tolerable
    "slow": "1s-3s",            # ğŸŸ  Poor experience
    "timeout": "> 3s"           # ğŸ”´ Unacceptable
}

SLO Targets:
  â€¢ 70% should be fast (< 500ms)
  â€¢ 95% should be under 1 second
  â€¢ 99.5% should be under 3 seconds
```

## Dashboard Widget Examples

### Widget 1: Histogram Visualization

```json
{
  "title": "EndpointGetTrace Latency Distribution",
  "type": "distribution",
  "requests": [{
    "q": "rpc.endpoint_timing{endpoint_name:EndpointGetTrace}",
    "style": {
      "palette": "dog_classic"
    }
  }],
  "xaxis": {
    "label": "Latency (ms)",
    "scale": "log"
  },
  "yaxis": {
    "label": "Request Count"
  }
}
```

### Widget 2: Bucket Trend Over Time

```json
{
  "title": "Requests by Latency Bucket (7d trend)",
  "type": "timeseries",
  "requests": [
    {
      "q": "sum:rpc.endpoint_timing{endpoint_name:EndpointGetTrace, rpc.endpoint_timing < 500}.as_count()",
      "display_type": "area",
      "style": {"palette": "green"}
    },
    {
      "q": "sum:rpc.endpoint_timing{endpoint_name:EndpointGetTrace, rpc.endpoint_timing >= 500 AND rpc.endpoint_timing < 1000}.as_count()",
      "display_type": "area",
      "style": {"palette": "orange"}
    },
    {
      "q": "sum:rpc.endpoint_timing{endpoint_name:EndpointGetTrace, rpc.endpoint_timing >= 1000}.as_count()",
      "display_type": "area",
      "style": {"palette": "red"}
    }
  ]
}
```

### Widget 3: SLO Compliance Gauge

```json
{
  "title": "SLO Compliance: % Requests < 500ms",
  "type": "query_value",
  "requests": [{
    "q": "(sum:rpc.endpoint_timing{endpoint_name:EndpointGetTrace, rpc.endpoint_timing < 500}.as_count() / sum:rpc.endpoint_timing{endpoint_name:EndpointGetTrace}.as_count()) * 100",
    "conditional_formats": [
      {"comparator": ">=", "value": 70, "palette": "white_on_green"},
      {"comparator": ">=", "value": 50, "palette": "white_on_yellow"},
      {"comparator": "<", "value": 50, "palette": "white_on_red"}
    ]
  }],
  "precision": 1,
  "unit": "%"
}
```

## Key Takeaways

âœ… **Distribution metrics enable:**
1. Custom bucket analysis
2. Percentage calculations (e.g., "% under 500ms")
3. Trend analysis by latency ranges
4. Identification of bimodal distributions
5. Precise SLO tracking

âœ… **Use cases:**
- Performance optimization targeting
- SLO compliance monitoring
- Identifying problematic segments
- Before/after comparisons
- Capacity planning

âœ… **Best practices:**
- Choose bucket boundaries based on your SLOs
- Monitor trends over time, not just current state
- Segment by meaningful dimensions (referrer, mode, etc.)
- Set alerts on both percentiles AND bucket percentages
- Review distribution regularly to catch drift

## Further Reading

- See `ENDPOINT_TIMING_METRICS.md` for complete metric reference
- See `CHANGES_SUMMARY.md` for implementation details
- DataDog Distribution Metrics: https://docs.datadoghq.com/metrics/distributions/
