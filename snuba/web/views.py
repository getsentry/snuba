import logging
import os
import time
from datetime import datetime
from typing import Any, Mapping, MutableMapping, MutableSequence, Tuple
from uuid import UUID

import jsonschema
import sentry_sdk
import simplejson as json
from flask import Flask, Response, redirect, render_template
from flask import request as http_request
from markdown import markdown

from snuba import environment, settings, state, util
from snuba.clickhouse.errors import ClickhouseError
from snuba.clickhouse.http import JSONRowEncoder
from snuba.clusters.cluster import ClickhouseClientSettings
from snuba.consumer import KafkaMessageMetadata
from snuba.datasets.dataset import Dataset
from snuba.datasets.factory import (
    InvalidDatasetError,
    enforce_table_writer,
    get_dataset,
    get_dataset_name,
    get_enabled_dataset_names,
)
from snuba.datasets.schemas.tables import TableSchema
from snuba.query.exceptions import InvalidQueryException
from snuba.redis import redis_client
from snuba.request.exceptions import InvalidJsonRequestException, JsonDecodeException
from snuba.request.request_settings import HTTPRequestSettings
from snuba.request.schema import RequestSchema
from snuba.request.validation import build_request
from snuba.state.rate_limit import RateLimitExceeded
from snuba.subscriptions.codecs import SubscriptionDataCodec
from snuba.subscriptions.data import InvalidSubscriptionError, PartitionId
from snuba.subscriptions.subscription import SubscriptionCreator, SubscriptionDeleter
from snuba.util import with_span
from snuba.utils.metrics.backends.wrapper import MetricsWrapper
from snuba.utils.metrics.timer import Timer
from snuba.utils.streams import Message, Partition, Topic
from snuba.utils.streams.backends.kafka import KafkaPayload
from snuba.web import QueryException
from snuba.web.converters import DatasetConverter
from snuba.web.query import parse_and_run_query
from snuba.writer import BatchWriterEncoderWrapper, WriterTableRow

metrics = MetricsWrapper(environment.metrics, "api")

logger = logging.getLogger("snuba.api")


try:
    import uwsgi
except ImportError:

    def check_down_file_exists() -> bool:
        return False


else:

    def check_down_file_exists() -> bool:
        try:
            return os.stat("/tmp/snuba.down").st_mtime > uwsgi.started_on
        except OSError:
            return False


def check_clickhouse() -> bool:
    """
    Checks if all the tables in all the enabled datasets exist in ClickHouse
    """
    try:
        for name in get_enabled_dataset_names():
            dataset = get_dataset(name)

            for storage in dataset.get_all_storages():
                clickhouse = storage.get_cluster().get_query_connection(
                    ClickhouseClientSettings.QUERY
                )
                clickhouse_tables = clickhouse.execute("show tables")
                source = storage.get_schema()
                if isinstance(source, TableSchema):
                    table_name = source.get_table_name()
                    if (table_name,) not in clickhouse_tables:
                        return False

        return True

    except Exception:
        return False


def truncate_dataset(dataset: Dataset) -> None:
    for storage in dataset.get_all_storages():
        cluster = storage.get_cluster()
        clickhouse = cluster.get_query_connection(ClickhouseClientSettings.MIGRATE)
        database = cluster.get_database()

        schema = storage.get_schema()

        if not isinstance(schema, TableSchema):
            return

        table = schema.get_local_table_name()

        clickhouse.execute(f"TRUNCATE TABLE IF EXISTS {database}.{table}")


application = Flask(__name__, static_url_path="")
application.testing = settings.TESTING
application.debug = settings.DEBUG
application.url_map.converters["dataset"] = DatasetConverter


@application.errorhandler(InvalidJsonRequestException)
def handle_invalid_json(exception: InvalidJsonRequestException) -> Response:
    cause = getattr(exception, "__cause__", None)
    if isinstance(cause, json.JSONDecodeError):
        data = {"error": {"type": "json", "message": str(cause)}}
    elif isinstance(cause, jsonschema.ValidationError):
        data = {
            "error": {
                "type": "schema",
                "message": cause.message,
                "path": list(cause.path),
                "schema": cause.schema,
            }
        }
    else:
        data = {"error": {"type": "request", "message": str(exception)}}

    def default_encode(value):
        # XXX: This is necessary for rendering schema defaults values that are
        # generated by callables, rather than constants.
        if callable(value):
            return value()
        else:
            raise TypeError()

    return Response(
        json.dumps(data, indent=4, default=default_encode),
        400,
        {"Content-Type": "application/json"},
    )


@application.errorhandler(InvalidDatasetError)
def handle_invalid_dataset(exception: InvalidDatasetError) -> Response:
    data = {"error": {"type": "dataset", "message": str(exception)}}
    return Response(
        json.dumps(data, sort_keys=True, indent=4),
        404,
        {"Content-Type": "application/json"},
    )


@application.errorhandler(InvalidQueryException)
def handle_invalid_query(exception: InvalidQueryException) -> Response:
    # TODO: Remove this logging as soon as the query validation code is
    # mature enough that we can trust it.
    logger.warning("Invalid query", exc_info=True)

    # TODO: Add special cases with more structure for specific exceptions
    # if needed.
    return Response(
        json.dumps(
            {"error": {"type": "invalid_query", "message": str(exception)}}, indent=4
        ),
        400,
        {"Content-Type": "application/json"},
    )


@application.route("/")
def root():
    with open("README.md") as f:
        return render_template("index.html", body=markdown(f.read()))


@application.route("/css/<path:path>")
def send_css(path):
    return application.send_static_file(os.path.join("css", path))


@application.route("/img/<path:path>")
@application.route("/snuba/web/static/img/<path:path>")
def send_img(path):
    return application.send_static_file(os.path.join("img", path))


@application.route("/dashboard")
@application.route("/dashboard.<fmt>")
def dashboard(fmt="html"):
    if fmt == "json":
        result = {
            "queries": state.get_queries(),
            "concurrent": {k: state.get_concurrent(k) for k in ["global"]},
            "rates": {k: state.get_rates(k) for k in ["global"]},
        }
        return (json.dumps(result), 200, {"Content-Type": "application/json"})
    else:
        return application.send_static_file("dashboard.html")


@application.route("/config")
@application.route("/config.<fmt>", methods=["GET", "POST"])
def config(fmt="html"):
    if fmt == "json":
        if http_request.method == "GET":
            return (
                json.dumps(state.get_raw_configs()),
                200,
                {"Content-Type": "application/json"},
            )
        elif http_request.method == "POST":
            state.set_configs(
                json.loads(http_request.data),
                user=http_request.headers.get("x-forwarded-email"),
            )
            return (
                json.dumps(state.get_raw_configs()),
                200,
                {"Content-Type": "application/json"},
            )
    else:
        return application.send_static_file("config.html")


@application.route("/config/changes.json")
def config_changes():
    return (
        json.dumps(state.get_config_changes()),
        200,
        {"Content-Type": "application/json"},
    )


@application.route("/health")
def health() -> Response:
    down_file_exists = check_down_file_exists()
    thorough = http_request.args.get("thorough", False)
    clickhouse_health = check_clickhouse() if thorough else True

    if not down_file_exists and clickhouse_health:
        body = {"status": "ok"}
        status = 200
    else:
        body = {
            "down_file_exists": down_file_exists,
        }
        if thorough:
            body["clickhouse_ok"] = clickhouse_health
        status = 502

    return Response(json.dumps(body), status, {"Content-Type": "application/json"})


def parse_request_body(http_request):
    with sentry_sdk.start_span(description="parse_request_body", op="parse"):
        metrics.timing("http_request_body_length", len(http_request.data))
        try:
            return json.loads(http_request.data)
        except json.JSONDecodeError as error:
            raise JsonDecodeException(str(error)) from error


def _trace_transaction(dataset: Dataset) -> None:
    with sentry_sdk.configure_scope() as scope:
        if scope.span:
            scope.span.set_tag("dataset", get_dataset_name(dataset))
            scope.span.set_tag("referrer", http_request.referrer)


@application.route("/query", methods=["GET", "POST"])
@util.time_request("query")
def unqualified_query_view(*, timer: Timer):
    if http_request.method == "GET":
        return redirect(f"/{settings.DEFAULT_DATASET_NAME}/query", code=302)
    elif http_request.method == "POST":
        body = parse_request_body(http_request)
        dataset = get_dataset(body.pop("dataset", settings.DEFAULT_DATASET_NAME))
        _trace_transaction(dataset)
        return dataset_query(dataset, body, timer)
    else:
        assert False, "unexpected fallthrough"


@application.route("/<dataset:dataset>/query", methods=["GET", "POST"])
@util.time_request("query")
def dataset_query_view(*, dataset: Dataset, timer: Timer):
    if http_request.method == "GET":
        schema = RequestSchema.build_with_extensions(
            dataset.get_extensions(), HTTPRequestSettings
        )
        return render_template(
            "query.html",
            query_template=json.dumps(schema.generate_template(), indent=4,),
        )
    elif http_request.method == "POST":
        body = parse_request_body(http_request)
        _trace_transaction(dataset)
        return dataset_query(dataset, body, timer)
    else:
        assert False, "unexpected fallthrough"


@with_span()
def dataset_query(dataset: Dataset, body, timer: Timer) -> Response:
    assert http_request.method == "POST"

    with sentry_sdk.start_span(description="build_schema", op="validate"):
        schema = RequestSchema.build_with_extensions(
            dataset.get_extensions(), HTTPRequestSettings
        )

    request = build_request(body, schema, timer, dataset, http_request.referrer)

    try:
        result = parse_and_run_query(dataset, request, timer)
    except QueryException as exception:
        status = 500
        details: Mapping[str, Any]

        cause = exception.__cause__
        if isinstance(cause, RateLimitExceeded):
            status = 429
            details = {
                "type": "rate-limited",
                "message": "rate limit exceeded",
            }
        elif isinstance(cause, ClickhouseError):
            details = {
                "type": "clickhouse",
                "message": str(cause),
                "code": cause.code,
            }
        elif isinstance(cause, Exception):
            details = {
                "type": "unknown",
                "message": str(cause),
            }
        else:
            raise  # exception should have been chained

        return Response(
            json.dumps(
                {"error": details, "timing": timer.for_json(), **exception.extra}
            ),
            status,
            {"Content-Type": "application/json"},
        )

    payload: MutableMapping[str, Any] = {**result.result, "timing": timer.for_json()}

    if settings.STATS_IN_RESPONSE or request.settings.get_debug():
        payload.update(result.extra)

    return Response(json.dumps(payload), 200, {"Content-Type": "application/json"})


@application.errorhandler(InvalidSubscriptionError)
def handle_subscription_error(exception: InvalidSubscriptionError) -> Response:
    data = {"error": {"type": "subscription", "message": str(exception)}}
    return Response(
        json.dumps(data, indent=4), 400, {"Content-Type": "application/json"},
    )


@application.route("/<dataset:dataset>/subscriptions", methods=["POST"])
@util.time_request("subscription")
def create_subscription(*, dataset: Dataset, timer: Timer):
    subscription = SubscriptionDataCodec().decode(http_request.data)
    # TODO: Check for valid queries with fields that are invalid for subscriptions. For
    # example date fields and aggregates.
    identifier = SubscriptionCreator(dataset).create(subscription, timer)
    return (
        json.dumps({"subscription_id": str(identifier)}),
        202,
        {"Content-Type": "application/json"},
    )


@application.route(
    "/<dataset:dataset>/subscriptions/<int:partition>/<key>", methods=["DELETE"]
)
def delete_subscription(*, dataset: Dataset, partition: int, key: str):
    SubscriptionDeleter(dataset, PartitionId(partition)).delete(UUID(key))
    return "ok", 202, {"Content-Type": "text/plain"}


if application.debug or application.testing:
    # These should only be used for testing/debugging. Note that the database name
    # is checked to avoid scary production mishaps.

    @application.route("/tests/<dataset:dataset>/insert", methods=["POST"])
    def write(*, dataset: Dataset):
        from snuba.processor import InsertBatch

        rows: MutableSequence[WriterTableRow] = []
        offset_base = int(round(time.time() * 1000))
        for index, message in enumerate(json.loads(http_request.data)):
            offset = offset_base + index
            processed_message = (
                enforce_table_writer(dataset)
                .get_stream_loader()
                .get_processor()
                .process_message(
                    message,
                    KafkaMessageMetadata(
                        offset=offset, partition=0, timestamp=datetime.utcnow()
                    ),
                )
            )
            if processed_message:
                assert isinstance(processed_message, InsertBatch)
                rows.extend(processed_message.rows)

        BatchWriterEncoderWrapper(
            enforce_table_writer(dataset).get_batch_writer(metrics), JSONRowEncoder(),
        ).write(rows)

        return ("ok", 200, {"Content-Type": "text/plain"})

    @application.route("/tests/<dataset:dataset>/eventstream", methods=["POST"])
    def eventstream(*, dataset: Dataset):
        record = json.loads(http_request.data)

        version = record[0]
        if version != 2:
            raise RuntimeError("Unsupported protocol version: %s" % record)

        message: Message[KafkaPayload] = Message(
            Partition(Topic("topic"), 0),
            0,
            KafkaPayload(None, http_request.data, []),
            datetime.now(),
        )

        type_ = record[1]

        storage = dataset.get_writable_storage()
        assert storage is not None

        if type_ == "insert":
            from snuba.consumer import ConsumerWorker

            worker = ConsumerWorker(storage, metrics=metrics)
        else:
            from snuba.replacer import ReplacerWorker

            worker = ReplacerWorker(storage, metrics=metrics)

        processed = worker.process_message(message)
        if processed is not None:
            batch = [processed]
            worker.flush_batch(batch)

        return ("ok", 200, {"Content-Type": "text/plain"})

    @application.route("/tests/<dataset:dataset>/drop", methods=["POST"])
    def drop(*, dataset: Dataset) -> Tuple[str, int, Mapping[str, str]]:
        truncate_dataset(dataset)
        redis_client.flushdb()

        return ("ok", 200, {"Content-Type": "text/plain"})

    @application.route("/tests/error")
    def error():
        1 / 0
